#!/usr/bin/env python2

# this daemon is in charge of communication between it and application instances
# and other head daemons in the job

import os
import sys
import copy
import tempfile
import subprocess
import signal
import socket
import re
import logging

errorcodes = {
	'MPI_SUCCESS': 0,
	'MPI_ABORT': 255,
	'MPI_FAULT': 124
	}

#def prefix_sum(array):
#	out = []
#
#	for idx, item in enumerate(array):
#		if idx == 0:
#			out.append(item)
#		else:
#			out.append(item + out[idx-1])
#
#	return out

#def lookup_setup():
#	# uncompress task list
#	prefixes = []
#	for element in os.environ['SLURM_TASKS_PER_NODE'].split(','):
#		if '(' in element:
#			# 143(x27)
#			count, mult = element.split('(x')
#			
#			for idx in xrange(int(mult[:-1])):
#				prefixes.append(count)
#			
#		else:
#			prefixes.append(int(element))
#	
#	# generate prefix sum array
#	global task_count_offsets 
#	task_count_offsets = prefix_sum(prefixes)

#def lookup_rank(rank):
#	if os.environ['EXAMPI_ENV'] == 'local':
#		return 'localhost'
#
#	# local rank
#	if rank in local_global_ids:
#		return local_nodename
#
#	# global rank 
#	else:
#		for idx, bound in enumerate(task_count_offsets):
#			if bound <= rank:
#				return nodelist[idx]

class HeadDaemon:
	def __init__(self, daemon_port, mpi_base_port):
		self.setup_logger()
		
		self.daemon_port = daemon_port
		self.shutdown = False

		self.setup_socket()
		
		# create signal handler
		signal.signal(signal.SIGINT, self.signal_handler)
		self.log.info('set up signal handler')
			
		# local data
		self.mpi_base_port = mpi_base_port
		self.rank_waiting_set = set()
		self.ranks_completed = set()
		self.local_rank_count = None
		self.local_ranks = []
		self.fault_daemons = []
		self.rank_pids = {}
		self.barrier_unannounced = True

		self.hostname = socket.gethostname()
		self.localhost = socket.gethostbyname(self.hostname)

		# global daemon
		self.root_node = None
		self.root_host = None
		self.node_waiting_set = set()
		self.global_node_count = None
		self.global_nodes = []

		self.world_size = int(os.environ['EXAMPI_WORLD_SIZE'])

		self.prepare_lookup()
	
		# generate message-function lookup
		self.generate_message_lookup()

	def __del__(self):
		self.log.info('shutting down socket')
		self.sock.close()
		self.log.info('shutting down')

	def setup_logger(self):
		if os.environ.get('EXAMPI_LOG_HEAD_DAEMON', None) == 'TRUE':
			self.log = logging.getLogger(__name__+'-'+str(os.environ['EXAMPI_RANK']))
			self.log.setLevel(logging.INFO)

			logname = 'head_daemon_' + str(os.environ['EXAMPI_RANK']) + '.log'

			handler = logging.FileHandler(logname)
			handler.setLevel(logging.INFO)

			self.log.addHandler(handler)

		else:
			self.log = logging.getLogger(__name__)
			self.log.addHandler(logging.NullHandler())

	def setup_socket(self):
		# create socket
		self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
		
		self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 0)
		self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 0)
		self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)

		self.sock.bind((socket.gethostname(), daemon_port))
		self.log.info('socket bound on ' + str(socket.gethostname()) + ' ' + str(socket.gethostbyname(socket.gethostname())) + ' ' + str(self.daemon_port))

	def generate_message_lookup(self):
		self.message_lookup = {}

		# fault daemons/application -> head daemon(1)
		self.message_lookup['barrier'] = self.process_barrier
		self.message_lookup['termination'] = self.process_termination
		#self.message_lookup['lookup'] = None # TODO dynamic connection building

		# head daemon(+) -> controller(1)
		self.message_lookup['global_barrier'] = self.process_global_barrier
		self.message_lookup['global_termination'] = self.process_global_termination
		
		# controller -> head daemons -> fault daemons(+)
		self.message_lookup['release'] = self.process_release
		#self.message_lookup['error'] = self.process_error
		self.message_lookup['shutdown'] = self.process_shutdown

	def signal_handler(self, signal, frame):
		self.log.warning("HEAD DAEMON signal received " + str(signal))
		self.shutdown = True

	def prepare_lookup(self):
		if os.environ['EXAMPI_ENV'] == 'local':
			self.local_rank_count = int(os.environ['EXAMPI_WORLD_SIZE'])
			self.local_ranks = range(int(os.environ['EXAMPI_WORLD_SIZE']))

			self.global_node_count = 1
			self.global_nodes = [socket.gethostname()]

			self.root_node = socket.gethostname()
			self.root_host = socket.gethostbyname(self.root_node)

		else:
			# TODO can we do this scheduler agnostic?
			
			# fill local ranks
			self.local_ranks = [int(gid) for gid in os.environ['SLURM_GTIDS'].split(',')]
			self.local_rank_count = len(self.local_ranks)
			
			command = ['scontrol']
	
			command.append('show')
			command.append('hostnames')
			command.append(os.environ['SLURM_JOB_NODELIST'])

			process = subprocess.Popen(command, stdout=subprocess.PIPE)
			stdout, stderr = process.communicate()

			self.global_nodes = stdout.splitlines()
			self.global_node_count = len(self.global_nodes)

			self.root_node = self.global_nodes[0] 
			self.root_host = socket.gethostbyname(self.root_node)

	def lookup_rank(self):
		if os.environ['EXAMPI_ENV'] == 'local':
			return self.root_node
		
		else:
			raise NotImplementedError

	def launch(self):
		while not self.shutdown:
			# receive 64B message
			data, addr = self.sock.recvfrom(64)	
			data = data.decode('utf-8').replace('\0','')
			self.log.info('raw data ' + str(type(data)) + ' ' + data)
			
			# process 
			self.log.info('processing: ' + str(data))
			# TODO don't redo split in each processing
			msg = data.split()[0]
			self.message_lookup[msg](data, addr)

	def send_to_controller(self, data):
		self.log.info('sending to controller: ' + str(data))

		self.sock.sendto(data, (self.root_node, self.daemon_port)) 

		self.log.info('finished sending to controller')

	def send_to_head_daemon(self, data, hostname):
		self.log.info('sending to head daemon on ' + hostname + ': ' + str(data))

		host = socket.gethostbyname(hostname)
		self.sock.sendto(data, (host, self.daemon_port))

		self.log.info('finished sending to head daemon')

	def send_to_head_daemons(self, data):
		self.log.info('sending ' + str(data) + ' to all head daemons')

		# send node release messages
		for hostname in self.node_waiting_set:
			self.send_to_head_daemon(data, hostname)

		self.log.info('finished sending to all head daemons')

	def send_to_rank(self, data, rank):
		self.log.info('sending to rank ' + str(rank) + ': ' + str(data))

		self.sock.sendto(data, (self.localhost, self.mpi_base_port + int(rank)))
		
		self.log.info('finished sending to ' + str(rank))
	
	def send_to_ranks(self, data):
		self.log.info('sending ' + str(data) + ' to all ranks')

		for rank in self.rank_waiting_set:
			self.send_to_rank(data, rank)

		self.log.info('finished sending to all ranks')

	def send_to_fault_daemon(self, data, rank):
		self.log.info('sending to fault daemon ' + str(rank) + ': ' + str(data))

		self.sock.sendto(data, (self.localhost, self.daemon_port-1-int(rank)))
		
		self.log.info('finished sending to fault daemon')
	
	def send_to_fault_daemons(self, data):
		self.log.info('sending ' + str(data) + ' to all fault daemons')

		for daemon in self.fault_daemons:
			self.send_to_fault_daemon(data, daemon)

		self.log.info('finished sending to all fault daemons')

	def process_barrier(self, data, addr):
		msg, rank, pid = data.split()

		rank = int(rank)
		pid = int(pid)

		# register rank for barrier 
		self.rank_waiting_set.add(rank)
		self.rank_pids[rank] = pid
		self.log.info('registered rank ' + str(rank) + ':' + str(pid) + ':waiting')
		
		# check if all local ranks are waiting
		if self.barrier_unannounced and len(self.rank_waiting_set) == self.local_rank_count:
			self.log.info('registered all local ranks, sending global barrier to controller')

			# flag to confirm we already sent message
			# TODO can this be done nicer?
			# TODO do we need this, we should never reach here again after all are waiting?
			self.barrier_unannounced = False

			msg = ('global_barrier ' + socket.gethostname() + '\0').ljust(64).encode('utf-8')

			self.send_to_controller(msg)

	def process_global_barrier(self, data, addr):
		msg, hostname = data.split()

		# register, if all, then send release messages
		self.node_waiting_set.add(hostname)
		self.log.info('registered node ' + hostname + ':waiting')

		self.log.info('check ' + str(len(self.node_waiting_set)) + ' == ' + str(self.global_node_count)) 
		if len(self.node_waiting_set) == self.global_node_count:
			self.log.info('complete global barrier, sending global release')

			# send node release messages
			data = 'release\0'.ljust(64).encode('utf-8')
			self.send_to_head_daemons(data)

	def process_release(self, data, addr):
		# forward release to all ranks
		self.log.info('present ranks ' + str(self.rank_waiting_set))

		msg = 'release\0'.ljust(64).encode('utf-8')
		self.send_to_ranks(msg)

	def process_termination(self, data, addr):
		msg, rank, errorcode, epoch = data.split()
		# this is from the fault daemon, representing the rank

		# register fault daemon
		self.fault_daemons.append(rank)

		msg = 'global_termination ' + self.hostname + ' ' + rank + ' ' + errorcode + ' ' + epoch + '\0'
		msg = msg.ljust(64)

		self.log.info('forwarding termination to controller: ' + msg)

		self.send_to_controller(msg.encode('utf-8'))

	def process_global_termination(self, data, addr):
		# head daemon to controller
		msg, hostname, rank, errorcode, epoch = data.split()

		rank = int(rank)
		errorcode = int(errorcode)
		epoch = int(epoch)

		if errorcode == errorcodes['MPI_SUCCESS']:
			self.log.info('rank ' + str(rank) + ' on ' + hostname + ' completed with MPI_SUCCESS')
			self.ranks_completed.add(rank)

			# exit once all local have completed
			self.log.info('check all ranks complete ' + str(len(self.ranks_completed)) + ' == ' + str(self.world_size))
			if len(self.ranks_completed) == self.world_size:
				self.log.info('all local ranks completed, sending shutdown')
				
				data = 'shutdown\0'.ljust(64).encode('utf-8')
				self.send_to_head_daemons(data)

		elif errorcode == errorcodes['MPI_ABORT']:
			# MPI_ABORT
			self.log.error('not implemented error MPI_ABORT')
			raise NotImplementedError

		else:
			self.log.error('not implemented error code: ' + str(data))
			raise NotImplementedError

	def process_shutdown(self, data, addr):
		self.log.info('received global shutdown from controller, shutting down...')
		self.shutdown = True

		self.log.info('sending shutdown to all fault daemons')
		data = 'shutdown\0'.ljust(64).encode('utf-8')
		self.send_to_fault_daemons(data)
		
	def process_rank_lookup(self, data, addr):
		# todo local rank is asking for remote address
		if os.environ['EXAMPI_ENV'] == 'local':
			return socket.gethostname()

		else:
			raise NotImplementedError
	
	def process_error(self, data, addr):
		# TODO inform all ranks of error for this head daemon
		raise NotImplementedError

if __name__ == '__main__':
	# todo read from environment
	daemon_port = int(os.environ["EXAMPI_DAEMON_PORT"])
	mpi_base_port = int(os.environ["EXAMPI_MPI_BASE_PORT"])

	#
	daemon = HeadDaemon(daemon_port, mpi_base_port)
	daemon.launch()
